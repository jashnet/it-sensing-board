import feedparser
from google import genai
from google.genai import types
from bs4 import BeautifulSoup
import json
import os
import re
from datetime import datetime, timedelta
import time
from deep_translator import GoogleTranslator
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed

# ì™¸ë¶€ í”„ë¡¬í”„íŠ¸
from prompts import DEFAULT_FILTER_PROMPT

# ğŸ’¡ Tier 1 ì£¼ìš” ë§¤ì²´ ë¦¬ìŠ¤íŠ¸ (MUST KNOW ê¶Œìœ„ íŒë³„ìš©)
TIER1_SOURCES = ['techcrunch', 'verge', 'wired', 'bloomberg', 'cnbc', 'wsj', 'reuters', 'engadget', 'nikkei', 'gizmodo', 'the information']

def load_prefs():
    pref_file = "learned_preferences.json"
    if os.path.exists(pref_file):
        try:
            with open(pref_file, "r", encoding="utf-8") as f: return json.load(f)
        except: return []
    return []

def run_morning_batch():
    print("ğŸŒ… [NGEPT ëª¨ë‹ ì„¼ì‹± V2] íŒŒì´í”„ë¼ì¸ ê°€ë™ ì‹œì‘...")
    
    api_key = os.environ.get("GEMINI_API_KEY", "").strip()
    if not api_key:
        print("ğŸš¨ ì—ëŸ¬: GEMINI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    client = genai.Client(api_key=api_key)

    try:
        with open("channels.json", "r", encoding="utf-8") as f: channels_data = json.load(f)
    except Exception as e:
        print(f"ğŸš¨ ì—ëŸ¬: channels.json ì½ê¸° ì‹¤íŒ¨ {e}")
        return

    limit = datetime.now() - timedelta(days=3)
    community_domains = ['reddit', 'v2ex', 'hacker news', 'ycombinator', 'clien', 'dcinside', 'blind']
    
    news_tasks = []
    comm_tasks = []
    
    for cat, feeds in channels_data.items():
        for f in feeds:
            if f.get("active", True):
                if any(d in f["url"].lower() for d in community_domains):
                    comm_tasks.append((cat, f, limit))
                else:
                    news_tasks.append((cat, f, limit))

    def fetch_worker(args):
        cat, f, lim = args
        articles = []
        try:
            d = feedparser.parse(f["url"])
            if not d.entries: return []
            # ğŸ’¡ [í•´ê²° 3] ìµœì‹  30ê°œê¹Œì§€ ê¸ì–´ì™€ ëª¨ìˆ˜ë¥¼ ìµœëŒ€í•œ ë„“í™ë‹ˆë‹¤.
            for entry in d.entries[:30]:
                dt = entry.get('published_parsed') or entry.get('updated_parsed')
                if not dt: continue
                p_date = datetime.fromtimestamp(time.mktime(dt))
                if p_date < lim: continue
                
                thumbnail = ""
                if 'media_content' in entry and len(entry.media_content) > 0: thumbnail = entry.media_content[0].get('url', '')
                elif 'media_thumbnail' in entry and len(entry.media_thumbnail) > 0: thumbnail = entry.media_thumbnail[0].get('url', '')
                if not thumbnail:
                    html_content = str(entry.get('content', [{}])[0].get('value', '')) + str(entry.get('summary', ''))
                    soup = BeautifulSoup(html_content, "html.parser")
                    img_tag = soup.find('img')
                    if img_tag and img_tag.get('src'): thumbnail = img_tag.get('src')

                articles.append({
                    "id": hashlib.md5(entry.link.encode()).hexdigest()[:12], 
                    "title_en": entry.title, 
                    "link": entry.link, 
                    "source": f["name"],
                    "category": cat, 
                    "date_obj": p_date.isoformat(), 
                    "date": p_date.strftime("%Y.%m.%d"),
                    "summary_en": BeautifulSoup(entry.get("summary", ""), "html.parser").get_text()[:300], 
                    "thumbnail": thumbnail
                })
        except: pass
        return articles

    # ==========================================
    # ğŸ“¡ TRACK A: ì»¤ë®¤ë‹ˆí‹° ì†Œì…œ ë¦¬ìŠ¤ë‹ (morning_buzz.json ìƒì„±)
    # ==========================================
    raw_comm = []
    print(f"ğŸ“¡ ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ìˆ˜ì§‘ ì¤‘... (ì±„ë„ {len(comm_tasks)}ê°œ)")
    with ThreadPoolExecutor(max_workers=10) as executor:
        for f in as_completed([executor.submit(fetch_worker, t) for t in comm_tasks]):
            raw_comm.extend(f.result())
            
    print(f"ğŸ’¬ ìˆ˜ì§‘ëœ ì»¤ë®¤ë‹ˆí‹° ê¸€: {len(raw_comm)}ê°œ. AI í•« í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘...")
    hot_buzz_keywords = []
    if raw_comm:
        # ìµœê·¼ 100ê°œ ê¸€ ì œëª©ì„ ë­‰ì³ì„œ AIì—ê²Œ ì „ë‹¬
        comm_titles = "\n".join([f"- {item['title_en']}" for item in raw_comm[:100]])
        buzz_prompt = f"ë‹¹ì‹ ì€ IT íŠ¸ë Œë“œ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” ì˜¤ëŠ˜ ìƒˆë²½ ê¸€ë¡œë²Œ ê¸±(Geek) ì»¤ë®¤ë‹ˆí‹°ì— ì˜¬ë¼ì˜¨ ê²Œì‹œê¸€ ì œëª©ë“¤ì…ë‹ˆë‹¤.\nì´ ì¤‘ì—ì„œ ê°€ì¥ ë§ì´ ì–¸ê¸‰ë˜ê³  í™”ì œê°€ ë˜ëŠ” íŠ¹ì • ê¸°ì—…, ì œí’ˆ, ê¸°ìˆ , í¼íŒ©í„° í‚¤ì›Œë“œ 15ê°œë¥¼ ì¶”ì¶œí•˜ì—¬ JSON ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œë§Œ ë°˜í™˜í•˜ì„¸ìš”.\n[ê²Œì‹œê¸€]\n{comm_titles}\n\n[ì¶œë ¥ í˜•ì‹]\n{{\"keywords\": [\"Apple\", \"AR Glass\", ...]}}"
        try:
            res = client.models.generate_content(model="gemini-2.5-flash", contents=buzz_prompt, config=types.GenerateContentConfig(response_mime_type="application/json"))
            json_match = re.search(r'\{.*\}', res.text.strip(), re.DOTALL)
            if json_match:
                hot_buzz_keywords = json.loads(json_match.group()).get("keywords", [])
                hot_buzz_keywords = [k.upper() for k in hot_buzz_keywords]
        except Exception as e: print(f"ë²„ì¦ˆ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    # ğŸ’¡ [í•´ê²° 5] ìˆ˜ë™ ì„¼ì‹±ì—ì„œë„ ì“¸ ìˆ˜ ìˆë„ë¡ Buzz íŒŒì¼ ë³„ë„ ì €ì¥!
    try:
        with open("morning_buzz.json", "w", encoding="utf-8") as f:
            json.dump({"date": datetime.now().isoformat(), "keywords": hot_buzz_keywords}, f, ensure_ascii=False)
        print(f"ğŸ”¥ morning_buzz.json ì €ì¥ ì™„ë£Œ (í•« í‚¤ì›Œë“œ: {len(hot_buzz_keywords)}ê°œ)")
    except Exception as e: print(f"ë²„ì¦ˆ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ==========================================
    # ğŸ“¡ TRACK B: ë‰´ìŠ¤ Pre-Filtering (ì´ˆë²Œ ì±„ì )
    # ==========================================
    raw_news = []
    print(f"ğŸ“¡ ê³µì‹ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì¤‘... (ì±„ë„ {len(news_tasks)}ê°œ)")
    with ThreadPoolExecutor(max_workers=20) as executor:
        for f in as_completed([executor.submit(fetch_worker, t) for t in news_tasks]):
            raw_news.extend(f.result())
            
    print(f"ğŸ“° ìˆ˜ì§‘ëœ ì „ì²´ ì›ë³¸ ê¸°ì‚¬: {len(raw_news)}ê°œ. (ì‹œê°„ìˆœ ë¬´ì‹í•œ ì»·ì˜¤í”„ íì§€!)")
    
    # ğŸ’¡ [í•´ê²° 3&4] ì‹œê°„ìˆœì´ ì•„ë‹Œ 'ì œëª© ê¸°ë°˜ Pre-filter' ì ìš© (ë‹¨ì–´ í•„í„°ë§ìœ¼ë¡œ 300ê°œ ì••ì¶• í›„ AI ë¶„ì„)
    learned_rules = load_prefs()
    rule_str = ", ".join(learned_rules)
    
    # 1ì°¨ ì´ˆìŠ¤í”¼ë“œ ë¡œì»¬ í…ìŠ¤íŠ¸ í•„í„°ë§ (ê°€ë²¼ìš´ ì—°ê´€ë„ ê²€ì‚¬)
    target_keywords = ['ai', 'apple', 'meta', 'google', 'wearable', 'ring', 'glass', 'robot', 'ux', 'release', 'launch']
    target_keywords.extend([r.lower() for r in rule_str.split()])
    
    for n in raw_news:
        text_lower = (n['title_en'] + " " + n['summary_en']).lower()
        n['pre_score'] = sum(2 for k in target_keywords if k in text_lower)
        # ğŸ’¡ [í•´ê²° 6] Tier 1 ë§¤ì²´ì—ëŠ” íƒœìƒì ìœ¼ë¡œ ê°•ë ¥í•œ ê°€ì  ë¶€ì—¬
        if any(t in n['source'].lower() for t in TIER1_SOURCES):
            n['pre_score'] += 10 
            n['is_tier1'] = True
        else:
            n['is_tier1'] = False
            
    # ì—°ê´€ë„ ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ 150ê°œë§Œ ë‚¨ê¸°ê¸° (ì—¬ê¸°ì„œ ì˜ì–‘ê°€ ì—†ëŠ” ê¸°ì‚¬ ëŒ€ê±° íƒˆë½)
    candidate_news = sorted(raw_news, key=lambda x: (x.get('pre_score', 0), x['date_obj']), reverse=True)[:150]
    print(f"âœ‚ï¸ ì œëª©/ë§¤ì²´ ì—°ê´€ë„ Pre-filter í†µê³¼ ê¸°ì‚¬: {len(candidate_news)}ê°œ")

    # ==========================================
    # ğŸ§  TRACK C: ì •ì˜ˆ 150ê°œ ê¸°ì‚¬ Deep Scoring
    # ==========================================
    base_prompt = DEFAULT_FILTER_PROMPT
    if learned_rules:
        rules_text = "\n".join([f"- {r}" for r in learned_rules])
        base_prompt += f"\n\n[ğŸš¨ ìµœìš°ì„  ê°€ì¤‘ì¹˜ (íŒ€ì¥ë‹˜ ì„ í˜¸ í•™ìŠµ ê·œì¹™)]\nì•„ë˜ ê·œì¹™ì— ë¶€í•©í•˜ëŠ” ê¸°ì‚¬ëŠ” ë°˜ë“œì‹œ ë†’ì€ ê°€ì‚°ì (80ì  ì´ìƒ)ì„ ë¶€ì—¬í•˜ì—¬ í•µì‹¬ ì´ìŠˆë¡œ ì„ ì •í•˜ì„¸ìš”:\n{rules_text}"

    processed_items = []
    
    def ai_scoring_worker(item):
        try:
            import random
            time.sleep(random.uniform(0.5, 1.5))
            score_query = f"{base_prompt}\n\n[í‰ê°€ ëŒ€ìƒ]\në§¤ì²´: {item['source']}\në§í¬: {item['link']}\nì œëª©: {item['title_en']}\nìš”ì•½: {item['summary_en'][:200]}"
            response = client.models.generate_content(model="gemini-2.5-flash", contents=score_query)
            
            json_match = re.search(r'\{.*\}', response.text.strip(), re.DOTALL)
            if json_match:
                parsed_data = json.loads(json_match.group())
                item['content_type'] = 'news'
                item['score'] = int(parsed_data.get('score', 0))
                item['insight_title'] = parsed_data.get('insight_title') or item['title_en']
                item['core_summary'] = parsed_data.get('core_summary') or item['summary_en'][:100]
                item['keywords'] = parsed_data.get('keywords', [])
                
                # ğŸ’¡ [í•´ê²° 6] Tier 1 ë§¤ì²´ + ë†’ì€ ì ìˆ˜ë©´ 'Headline' ë“±ê¸‰ ë¶€ì—¬
                if item.get('is_tier1') and item['score'] >= 80:
                    item['score'] = min(100, item['score'] + 5) # ìµœì¢… ë¶€ìŠ¤íŒ…
            else: raise ValueError("No JSON")
        except:
            item['content_type'] = 'news'
            item['score'] = 40 # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì ìˆ˜ í•˜í–¥
            item['insight_title'] = item['title_en']
            item['core_summary'] = item['summary_en'][:100]
            item['keywords'] = []
            
        try:
            item['insight_title'] = GoogleTranslator(source='auto', target='ko').translate(item['insight_title'])
            item['core_summary'] = GoogleTranslator(source='auto', target='ko').translate(item['core_summary'])
        except: pass
        return item

    print("ğŸ§  ì •ì˜ˆ ê¸°ì‚¬ 150ê°œ AI ì‹¬ì¸µ ì±„ì  ì‹œì‘...")
    with ThreadPoolExecutor(max_workers=5) as executor:
        for i, future in enumerate(as_completed({executor.submit(ai_scoring_worker, item): item for item in candidate_news})):
            processed_items.append(future.result())

    # ==========================================
    # ğŸ¯ TRACK D: ì†Œì…œ ë²„ì¦ˆ ìœµí•© & í¼ë¸”ë¦¬ì‹±
    # ==========================================
    final_pool = []
    for item in processed_items:
        news_kws = set([str(k).upper() for k in item.get('keywords', [])])
        overlap = news_kws.intersection(set(hot_buzz_keywords))
        if overlap:
            item['score'] = min(100, item['score'] + (len(overlap) * 5))
            item['community_buzz'] = True
            item['buzz_words'] = list(overlap)
        else:
            item['community_buzz'] = False
        final_pool.append(item)

    final_pool = sorted(final_pool, key=lambda x: x.get('score', 0), reverse=True)
    
    today_str = datetime.now().strftime("%Y-%m-%d")
    archive_dir = "archive"
    if not os.path.exists(archive_dir): os.makedirs(archive_dir)
        
    try:
        with open("today_news.json", "w", encoding="utf-8") as f:
            json.dump(final_pool, f, ensure_ascii=False, indent=4)
        with open(f"{archive_dir}/morning_sensing_{today_str}.json", "w", encoding="utf-8") as f:
            json.dump(final_pool, f, ensure_ascii=False, indent=4)
        print("âœ… ëª¨ë“  íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ë° ë°ì´í„° ì €ì¥ ì„±ê³µ!")
    except Exception as e:
        print(f"ğŸš¨ ì €ì¥ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    run_morning_batch()
