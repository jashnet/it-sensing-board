import feedparser
from google import genai
from bs4 import BeautifulSoup
import json
import os
import re
from datetime import datetime, timedelta
import time
from deep_translator import GoogleTranslator
import requests
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed

# ğŸ’¡ GitHub Secretsì—ì„œ API í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
API_KEY = os.environ.get("GEMINI_API_KEY")

DEFAULT_FILTER_PROMPT = """ê·€í•˜ëŠ” ì‚¼ì„±ì „ì ì°¨ì„¸ëŒ€ê²½í—˜ê¸°íšíŒ€ì˜ ë‰´ìŠ¤ í•„í„°ë§ AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë‰´ìŠ¤ì˜ ì œëª©ê³¼ ìš”ì•½ì„ ë³´ê³ , 2~3ë…„ ë’¤ ì¶œì‹œí•  'ì†Œë¹„ì ì¤‘ì‹¬ì˜ ì°¨ì„¸ëŒ€ ìŠ¤ë§ˆíŠ¸ ë””ë°”ì´ìŠ¤ ë° UX ê¸°íš'ì— ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œ ì‹œê·¸ë„ì¸ì§€ 0~100ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

[ìš°ì„ ìˆœìœ„ ê°€ì¤‘ì¹˜ ê·œì¹™]
- +ê°€ì¤‘ì¹˜: ì œí’ˆ/ë””ë°”ì´ìŠ¤ ë˜ëŠ” ëª…í™•í•œ ì•±/ì„œë¹„ìŠ¤ ì¤‘ì‹¬ë‚´ìš©, AI ê¸°ìˆ ì´ ê²°í•©ëœ ê²½í—˜ ë³€í™”, ìƒíƒœê³„ ì „ë°˜ì„ í”ë“œëŠ” íŒŒê¸‰ë ¥, ì£¼ìš” ë¹…í…Œí¬(Apple, MS, Meta, Google, OpenAI ë“±)ì˜ í•µì‹¬ ë™í–¥, ë¯¸êµ­ì— ë„ì „í•˜ëŠ” ì¤‘êµ­ì˜ ê·¹ë‹¨ì  í•˜ë“œì›¨ì–´/AI ë³€í˜• ì‹œë„.
- -ê°ì /ë°°ì œ: ë‹¨ìˆœ ì‹¤ì /ì¬ë¬´ ë°œí‘œ, ì •ì±…/ë²•ë¥ /íŠ¹í—ˆ ì†Œì†¡, ê¸°ì—… ì¸ì‚¬ ë™ì •, ê´‘ê³ ì„± ì´ë²¤íŠ¸, ìˆœìˆ˜ B2B/ì‚°ì—…ìš© ê¸°ìˆ , ë‹¨ìˆœ ë°ì´í„° ê´€ë ¨ ì •ë³´
- ì¡°ê±´ë¶€ í—ˆìš©: ìë™ì°¨, ì´ë™ìˆ˜ë‹¨, ìŠ¤ë§ˆíŠ¸í™ˆ ë“±ì€ ê·¸ ìì²´ë¡œëŠ” ì ìˆ˜ê°€ ë‚®ìœ¼ë‚˜, 'ìŠ¤ë§ˆíŠ¸ ë””ë°”ì´ìŠ¤(í°, ì›¨ì–´ëŸ¬ë¸”)ì™€ì˜ ì—°ë™ì„ í†µí•œ ìƒˆë¡œìš´ UX ì°½ì¶œ' ë‚´ìš©ì´ë¼ë©´ ë†’ì€ ì ìˆ˜ë¥¼ ë¶€ì—¬í•¨.

[ì¶œë ¥ í˜•ì‹ - ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•  ê²ƒ]
{
    "score": [0~100 ì‚¬ì´ì˜ ì •ìˆ˜],
    "insight_title": "[ì›ë¬¸ ë²ˆì—­ì´ ì•„ë‹Œ, ì°¨ì„¸ëŒ€ê²½í—˜ê¸°íšíŒ€ ê¸°íšì ê´€ì ì—ì„œ ë°”ë¼ë³¸ ì˜ë¯¸ í•´ì„ì„ ë‹´ì€ ë§¤ë ¥ì ì¸ 1ì¤„ ì¸ì‚¬ì´íŠ¸ ì œëª©(í•œêµ­ì–´)]",
    "core_summary": "[ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš©ì´ ë¬´ì—‡ì¸ì§€ íŒ©íŠ¸ ìœ„ì£¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆëŠ” 2~3ì¤„ ìš”ì•½(í•œêµ­ì–´)]"
}
"""

def safe_translate(text):
    if not text: return ""
    try: return GoogleTranslator(source='auto', target='ko').translate(text)
    except: return text

def fetch_raw_news(args):
    cat, f, limit = args
    articles = []
    try:
        d = feedparser.parse(f["url"])
        for entry in d.entries[:15]:
            dt = entry.get('published_parsed') or entry.get('updated_parsed')
            if not dt: continue
            p_date = datetime.fromtimestamp(time.mktime(dt))
            if p_date < limit: continue
            
            thumbnail = ""
            if 'media_content' in entry and len(entry.media_content) > 0:
                thumbnail = entry.media_content[0].get('url', '')
            elif 'media_thumbnail' in entry and len(entry.media_thumbnail) > 0:
                thumbnail = entry.media_thumbnail[0].get('url', '')
                
            articles.append({
                "id": hashlib.md5(entry.link.encode()).hexdigest()[:12],
                "title_en": entry.title, "link": entry.link, "source": f["name"],
                "category": cat, "date_obj": p_date.isoformat(), # JSON ì €ì¥ì„ ìœ„í•´ ë¬¸ìì—´ ë³€í™˜
                "date": p_date.strftime("%Y.%m.%d"),
                "summary_en": BeautifulSoup(entry.get("summary", ""), "html.parser").get_text()[:300],
                "thumbnail": thumbnail
            })
    except: pass
    return articles

def run_batch_sensing():
    print("ğŸš€ ë°°ì¹˜ ì„¼ì‹± ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    if not API_KEY:
        print("âŒ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    # ì±„ë„ ë¡œë“œ
    try:
        with open("channels.json", "r", encoding="utf-8") as f:
            channels_data = json.load(f)
    except:
        print("âŒ channels.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì„¤ì •ê°’ (ê³ ì •)
    sensing_period = 3
    max_articles = 60
    filter_weight = 70
    limit = datetime.now() - timedelta(days=sensing_period)

    active_tasks = []
    for cat, feeds in channels_data.items():
        for f in feeds:
            if f.get("active", True):
                active_tasks.append((cat, f, limit))

    raw_news = []
    print(f"ğŸ“¡ {len(active_tasks)}ê°œ ì±„ë„ì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
    with ThreadPoolExecutor(max_workers=40) as executor:
        futures = [executor.submit(fetch_raw_news, t) for t in active_tasks]
        for f in as_completed(futures): raw_news.extend(f.result())

    raw_news = sorted(raw_news, key=lambda x: x['date_obj'], reverse=True)[:max_articles]
    
    client = genai.Client(api_key=API_KEY)
    filtered_list = []

    def ai_scoring_worker(item):
        try:
            score_query = f"{DEFAULT_FILTER_PROMPT}\n\n[í‰ê°€ ëŒ€ìƒ]\nì œëª©: {item['title_en']}\nìš”ì•½: {item['summary_en'][:200]}"
            response = client.models.generate_content(model="gemini-2.5-flash", contents=score_query)
            res = response.text.strip()
            if res.startswith("```json"): res = res[7:-3].strip()
            elif res.startswith("```"): res = res[3:-3].strip()
            
            parsed_data = json.loads(res)
            item['score'] = int(parsed_data.get('score', 50))
            item['insight_title'] = parsed_data.get('insight_title') or safe_translate(item['title_en'])
            item['core_summary'] = parsed_data.get('core_summary') or safe_translate(item['summary_en'])
        except Exception:
            item['score'] = 50 
            item['insight_title'] = safe_translate(item['title_en'])
            item['core_summary'] = safe_translate(item['summary_en'])
        return item

    print(f"ğŸ§  {len(raw_news)}ê°œ ê¸°ì‚¬ì— ëŒ€í•´ AI í•„í„°ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    with ThreadPoolExecutor(max_workers=30) as executor:
        future_to_item = {executor.submit(ai_scoring_worker, item): item for item in raw_news}
        for future in as_completed(future_to_item):
            item = future.result()
            if item['score'] >= filter_weight:
                filtered_list.append(item)

    final_news = sorted(filtered_list, key=lambda x: x.get('score', 0), reverse=True)
    
    # ğŸ’¾ ê²°ê³¼ë¬¼ì„ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    with open("today_news.json", "w", encoding="utf-8") as f:
        json.dump(final_news, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… ë°°ì¹˜ ì‘ì—… ì™„ë£Œ! ì´ {len(final_news)}ê°œì˜ ê¸°ì‚¬ê°€ today_news.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    run_batch_sensing()
