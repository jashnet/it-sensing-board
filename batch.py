import feedparser
from google import genai
from bs4 import BeautifulSoup
import json
import os
import re
from datetime import datetime, timedelta
import time
from deep_translator import GoogleTranslator
import requests
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed

# ğŸ‘‡ ìƒˆë¡­ê²Œ ì¶”ê°€ëœ ë§ˆë²•ì˜ 1ì¤„! (ì—¬ê¸°ì—” í•„í„° í”„ë¡¬í”„íŠ¸ë§Œ í•„ìš”í•©ë‹ˆë‹¤)
from prompts import DEFAULT_FILTER_PROMPT

# ğŸ’¡ GitHub Secretsì—ì„œ API í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
API_KEY = os.environ.get("GEMINI_API_KEY")

# ==========================================
# âŒ [ê¸°ì¡´ ì½”ë“œ ì‚­ì œ] ì•„ë˜ì— ìˆë˜ ê¸¸ê³  ê¸´ 
# DEFAULT_FILTER_PROMPT í…ìŠ¤íŠ¸ ë©ì–´ë¦¬ë¥¼ 
# ì „ë¶€ í†µì§¸ë¡œ ì§€ì›Œì£¼ì„¸ìš”! 
# ==========================================

def safe_translate(text):
    if not text: return ""
    try: return GoogleTranslator(source='auto', target='ko').translate(text)
    except: return text

def fetch_raw_news(args):
    cat, f, limit = args
    articles = []
    try:
        d = feedparser.parse(f["url"])
        for entry in d.entries[:15]:
            dt = entry.get('published_parsed') or entry.get('updated_parsed')
            if not dt: continue
            p_date = datetime.fromtimestamp(time.mktime(dt))
            if p_date < limit: continue
            
            thumbnail = ""
            if 'media_content' in entry and len(entry.media_content) > 0:
                thumbnail = entry.media_content[0].get('url', '')
            elif 'media_thumbnail' in entry and len(entry.media_thumbnail) > 0:
                thumbnail = entry.media_thumbnail[0].get('url', '')
                
            articles.append({
                "id": hashlib.md5(entry.link.encode()).hexdigest()[:12],
                "title_en": entry.title, "link": entry.link, "source": f["name"],
                "category": cat, "date_obj": p_date.isoformat(), # JSON ì €ì¥ì„ ìœ„í•´ ë¬¸ìì—´ ë³€í™˜
                "date": p_date.strftime("%Y.%m.%d"),
                "summary_en": BeautifulSoup(entry.get("summary", ""), "html.parser").get_text()[:300],
                "thumbnail": thumbnail
            })
    except: pass
    return articles

def run_batch_sensing():
    print("ğŸš€ ë°°ì¹˜ ì„¼ì‹± ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    if not API_KEY:
        print("âŒ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    # ì±„ë„ ë¡œë“œ
    try:
        with open("channels.json", "r", encoding="utf-8") as f:
            channels_data = json.load(f)
    except:
        print("âŒ channels.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì„¤ì •ê°’ (ê³ ì •)
    sensing_period = 3
    max_articles = 60
    filter_weight = 70
    limit = datetime.now() - timedelta(days=sensing_period)

    active_tasks = []
    for cat, feeds in channels_data.items():
        for f in feeds:
            if f.get("active", True):
                active_tasks.append((cat, f, limit))

    raw_news = []
    print(f"ğŸ“¡ {len(active_tasks)}ê°œ ì±„ë„ì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
    with ThreadPoolExecutor(max_workers=40) as executor:
        futures = [executor.submit(fetch_raw_news, t) for t in active_tasks]
        for f in as_completed(futures): raw_news.extend(f.result())

    raw_news = sorted(raw_news, key=lambda x: x['date_obj'], reverse=True)[:max_articles]
    
    client = genai.Client(api_key=API_KEY)
    filtered_list = []

    def ai_scoring_worker(item):
        try:
            score_query = f"{DEFAULT_FILTER_PROMPT}\n\n[í‰ê°€ ëŒ€ìƒ]\nì œëª©: {item['title_en']}\nìš”ì•½: {item['summary_en'][:200]}"
            response = client.models.generate_content(model="gemini-2.5-flash", contents=score_query)
            res = response.text.strip()
            if res.startswith("```json"): res = res[7:-3].strip()
            elif res.startswith("```"): res = res[3:-3].strip()
            
            parsed_data = json.loads(res)
            item['score'] = int(parsed_data.get('score', 50))
            item['insight_title'] = parsed_data.get('insight_title') or safe_translate(item['title_en'])
            item['core_summary'] = parsed_data.get('core_summary') or safe_translate(item['summary_en'])
        except Exception:
            item['score'] = 50 
            item['insight_title'] = safe_translate(item['title_en'])
            item['core_summary'] = safe_translate(item['summary_en'])
        return item

    print(f"ğŸ§  {len(raw_news)}ê°œ ê¸°ì‚¬ì— ëŒ€í•´ AI í•„í„°ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    with ThreadPoolExecutor(max_workers=30) as executor:
        future_to_item = {executor.submit(ai_scoring_worker, item): item for item in raw_news}
        for future in as_completed(future_to_item):
            item = future.result()
            if item['score'] >= filter_weight:
                filtered_list.append(item)

    final_news = sorted(filtered_list, key=lambda x: x.get('score', 0), reverse=True)
    
    # ğŸ’¾ ê²°ê³¼ë¬¼ì„ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    with open("today_news.json", "w", encoding="utf-8") as f:
        json.dump(final_news, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… ë°°ì¹˜ ì‘ì—… ì™„ë£Œ! ì´ {len(final_news)}ê°œì˜ ê¸°ì‚¬ê°€ today_news.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    run_batch_sensing()
