import feedparser
from google import genai
from bs4 import BeautifulSoup
import json
import os
import re
from datetime import datetime, timedelta
import time
from deep_translator import GoogleTranslator
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import Counter

# ì™¸ë¶€ í”„ë¡¬í”„íŠ¸
from prompts import DEFAULT_FILTER_PROMPT

# ğŸ’¡ ì¶”ê°€ë¨: íŒ€ì¥ë‹˜ì˜ ì„ í˜¸ í•™ìŠµ ê·œì¹™ì„ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
def load_prefs():
    pref_file = "learned_preferences.json"
    if os.path.exists(pref_file):
        try:
            with open(pref_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return []
    return []

def run_morning_batch():
    print("ğŸŒ… [ëª¨ë‹ ì„¼ì‹±] ìë™í™” ë´‡ ì‘ë™ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # 1. GitHub Secrets ë“± í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    api_key = os.environ.get("GEMINI_API_KEY", "").strip()
    if not api_key:
        print("ğŸš¨ ì—ëŸ¬: GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ì±„ë„ íŒŒì¼ ì½ê¸°
    try:
        with open("channels.json", "r", encoding="utf-8") as f:
            channels_data = json.load(f)
    except Exception as e:
        print(f"ğŸš¨ ì—ëŸ¬: channels.json íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {e}")
        return

    # ìµœê·¼ 3ì¼ì¹˜ ê¸°ì‚¬ë§Œ 1ì°¨ ìˆ˜ì§‘
    limit = datetime.now() - timedelta(days=3)
    active_tasks = []
    for cat, feeds in channels_data.items():
        for f in feeds:
            if f.get("active", True):
                active_tasks.append((cat, f, limit))

    raw_news = []
    
    def fetch_worker(args):
        cat, f, lim = args
        articles = []
        try:
            d = feedparser.parse(f["url"])
            if not d.entries: return []
            for entry in d.entries[:15]:
                dt = entry.get('published_parsed') or entry.get('updated_parsed')
                if not dt: continue
                p_date = datetime.fromtimestamp(time.mktime(dt))
                if p_date < lim: continue
                
                thumbnail = ""
                if 'media_content' in entry and len(entry.media_content) > 0: thumbnail = entry.media_content[0].get('url', '')
                elif 'media_thumbnail' in entry and len(entry.media_thumbnail) > 0: thumbnail = entry.media_thumbnail[0].get('url', '')
                if not thumbnail:
                    html_content = str(entry.get('content', [{}])[0].get('value', '')) + str(entry.get('summary', ''))
                    soup = BeautifulSoup(html_content, "html.parser")
                    img_tag = soup.find('img')
                    if img_tag and img_tag.get('src'): thumbnail = img_tag.get('src')

                articles.append({
                    "id": hashlib.md5(entry.link.encode()).hexdigest()[:12], 
                    "title_en": entry.title, 
                    "link": entry.link, 
                    "source": f["name"],
                    "category": cat, 
                    "date_obj": p_date.isoformat(), 
                    "date": p_date.strftime("%Y.%m.%d"),
                    "summary_en": BeautifulSoup(entry.get("summary", ""), "html.parser").get_text()[:300], 
                    "thumbnail": thumbnail
                })
        except: pass
        return articles

    print(f"ğŸ“¡ {len(active_tasks)}ê°œì˜ ì±„ë„ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
    with ThreadPoolExecutor(max_workers=20) as executor:
        for f in as_completed([executor.submit(fetch_worker, t) for t in active_tasks]):
            raw_news.extend(f.result())
            
    # AI í• ë‹¹ëŸ‰ ê´€ë¦¬ë¥¼ ìœ„í•´ 100ê°œë§Œ ìë¥´ê¸°
    raw_news = sorted(raw_news, key=lambda x: x['date_obj'], reverse=True)[:100]
    print(f"âœ… ì´ {len(raw_news)}ê°œ ê¸°ì‚¬ 1ì°¨ í™•ë³´ ì™„ë£Œ. AI ì±„ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    # ğŸ’¡ğŸ’¡ğŸ’¡ í•µì‹¬ ì¶”ê°€: í•™ìŠµëœ ì„ í˜¸ ê¸°ì‚¬ ê·œì¹™ ë³‘í•©
    base_prompt = DEFAULT_FILTER_PROMPT
    learned_rules = load_prefs()
    if learned_rules:
        print(f"ğŸ§  [RLHF] íŒ€ì¥ë‹˜ì´ ì§€ì‹œí•œ {len(learned_rules)}ê°œì˜ í•™ìŠµ ê·œì¹™ì„ AIì˜ ë‘ë‡Œì— ì£¼ì…í•©ë‹ˆë‹¤.")
        rules_text = "\n".join([f"- {r}" for r in learned_rules])
        base_prompt += f"\n\n[ğŸš¨ ìµœìš°ì„  ê°€ì¤‘ì¹˜ (íŒ€ì¥ë‹˜ ì„ í˜¸ í•™ìŠµ ê·œì¹™)]\nì•„ë˜ ê·œì¹™ì— ë¶€í•©í•˜ëŠ” ê¸°ì‚¬ëŠ” ë°˜ë“œì‹œ ë†’ì€ ê°€ì‚°ì (80ì  ì´ìƒ)ì„ ë¶€ì—¬í•˜ì—¬ í•µì‹¬ ì´ìŠˆë¡œ ì„ ì •í•˜ì„¸ìš”:\n{rules_text}"
    else:
        print("â„¹ï¸ ì ìš©ëœ ì¶”ê°€ í•™ìŠµ ê·œì¹™ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")

    client = genai.Client(api_key=api_key)
    processed_items = []
    
    def ai_scoring_worker(item):
        try:
            import random
            time.sleep(random.uniform(0.5, 1.5)) # API ì œí•œ íšŒí”¼
            
            # ğŸ’¡ ìˆ˜ì •ë¨: DEFAULT_FILTER_PROMPT ëŒ€ì‹  ê·œì¹™ì´ ë³‘í•©ëœ base_prompt ì‚¬ìš©
            score_query = f"{base_prompt}\n\n[í‰ê°€ ëŒ€ìƒ]\në§¤ì²´: {item['source']}\në§í¬: {item['link']}\nì œëª©: {item['title_en']}\nìš”ì•½: {item['summary_en'][:200]}"
            response = client.models.generate_content(model="gemini-2.5-flash", contents=score_query)
            
            json_match = re.search(r'\{.*\}', response.text.strip(), re.DOTALL)
            if json_match:
                parsed_data = json.loads(json_match.group())
                url_lower = item['link'].lower()
                source_lower = item['source'].lower()
                community_domains = ['reddit', 'v2ex', 'hacker news', 'ycombinator', 'clien', 'dcinside', 'blind']
                
                if any(d in url_lower or d in source_lower for d in community_domains):
                    item['content_type'] = 'community'
                else:
                    item['content_type'] = parsed_data.get('content_type', 'news')
                
                item['score'] = int(parsed_data.get('score', 0)) if item['content_type'] == 'news' else 0
                item['insight_title'] = parsed_data.get('insight_title') or item['title_en']
                item['core_summary'] = parsed_data.get('core_summary') or item['summary_en'][:100]
                item['keywords'] = parsed_data.get('keywords', [])
            else: raise ValueError("No JSON")
        except:
            item['content_type'] = 'news'
            item['score'] = 50 
            item['insight_title'] = item['title_en']
            item['core_summary'] = item['summary_en'][:100]
            item['keywords'] = []
            
        # ì˜ë¬¸ ë²ˆì—­ (ë¬´ë£Œ ë²ˆì—­ê¸° í•œë„ ìš°íšŒ)
        try:
            item['insight_title'] = GoogleTranslator(source='auto', target='ko').translate(item['insight_title'])
            item['core_summary'] = GoogleTranslator(source='auto', target='ko').translate(item['core_summary'])
        except: pass
        
        return item

    with ThreadPoolExecutor(max_workers=5) as executor:
        for i, future in enumerate(as_completed({executor.submit(ai_scoring_worker, item): item for item in raw_news})):
            processed_items.append(future.result())
            print(f"ğŸ§  ë¶„ì„ ì§„í–‰ ì¤‘... ({i+1}/{len(raw_news)})")

    # ì†Œì…œ ë¦¬ìŠ¤ë‹ ë²„ì¦ˆ ë°˜ì˜
    community_keywords = []
    for item in processed_items:
        if item.get('content_type') == 'community':
            community_keywords.extend([str(k).upper() for k in item.get('keywords', [])])
            
    comm_kw_counts = Counter(community_keywords)
    hot_comm_keywords = set([k for k, v in comm_kw_counts.items() if v >= 1])

    final_pool = []
    for item in processed_items:
        if item.get('content_type') == 'news':
            news_kws = set([str(k).upper() for k in item.get('keywords', [])])
            overlap = news_kws.intersection(hot_comm_keywords)
            if overlap:
                item['score'] = min(100, item['score'] + (len(overlap) * 5))
                item['community_buzz'] = True
                item['buzz_words'] = list(overlap)
            else:
                item['community_buzz'] = False
            final_pool.append(item)

    final_pool = sorted(final_pool, key=lambda x: x.get('score', 0), reverse=True)
    
    # 3. íŒŒì¼ ì €ì¥ (today_news.jsonì€ ì•±ì´ ë°”ë¡œ ì½ì„ ìš©ë„, archiveëŠ” ë‚ ì§œë³„ ê¸°ë¡ ìš©ë„)
    today_str = datetime.now().strftime("%Y-%m-%d")
    archive_dir = "archive"
    if not os.path.exists(archive_dir):
        os.makedirs(archive_dir)
        
    try:
        with open("today_news.json", "w", encoding="utf-8") as f:
            json.dump(final_pool, f, ensure_ascii=False, indent=4)
        print("âœ… today_news.json ì €ì¥ ì™„ë£Œ")
        
        with open(f"{archive_dir}/morning_sensing_{today_str}.json", "w", encoding="utf-8") as f:
            json.dump(final_pool, f, ensure_ascii=False, indent=4)
        print(f"âœ… ì•„ì¹´ì´ë¸Œ ì €ì¥ ì™„ë£Œ: morning_sensing_{today_str}.json")
    except Exception as e:
        print(f"ğŸš¨ ì €ì¥ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    run_morning_batch()
